{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f90140-9e32-4da6-871b-8138a55aa36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bb51edc-e3dc-45d5-980c-8384c356d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.rand(4,6)\n",
    "ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "ids_restore = torch.argsort(ids_shuffle, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9735f984-3a09-4f09-b0db-5f6f95cc40d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 2, 5, 0, 1, 4],\n",
       "        [0, 2, 4, 1, 5, 3],\n",
       "        [2, 0, 5, 1, 3, 4],\n",
       "        [4, 3, 0, 1, 2, 5]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98dca5e-19c2-48ed-84a2-31e427dd8a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 4, 1, 0, 5, 2],\n",
       "        [0, 3, 1, 5, 2, 4],\n",
       "        [1, 3, 0, 4, 5, 2],\n",
       "        [2, 3, 4, 1, 0, 5]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cf30c69-6ca7-4fc1-9a3c-40b9ba877d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 2],\n",
       "        [0, 2],\n",
       "        [2, 0],\n",
       "        [4, 3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_shuffle[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d018770-c1e9-438d-937c-f3bec93715e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.ones([4, 6])\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e28360d-2ba8-4db2-a224-69e041633e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[:, :2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58403e09-de3d-4bfe-8c80-0cdd2eb7c515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3979bdfa-32e4-4f72-969a-036eae4e7f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 1., 1.],\n",
       "        [0., 1., 0., 1., 1., 1.],\n",
       "        [0., 1., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 0., 0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c241140-fb9c-4147-8022-542742d0f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c251e06c-c05b-4060-b236-9f46e0903353",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = nn.Linear(128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b3fd6ae-75a5-4fd6-b49e-0eaaa4da0185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 2, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k(torch.rand(2,4,2,128)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd1cdefd-655d-4818-bc15-92842c19322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "from Model.Module.asymAttention import AsymAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd8310c0-68c2-4659-98f3-5e1acb89a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Block(128, 4, qkv_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c69811b1-0ad6-4d87-b577-51e882127e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.attn = AsymAttention(\n",
    "    dim=128,\n",
    "    num_heads=4,\n",
    "    qkv_bias=True,\n",
    "    attn_drop=0.,\n",
    "    proj_drop=0.,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5dd9d9b-e0f7-479e-b188-23c3d17e20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PatchEmbed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45d639d4-a4e5-4814-8d55-7d428eda72c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatchEmbed(\n",
       "  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (norm): Identity()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c70be8fa-47c1-49e7-b228-a7388070ca5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Block(\n",
       "  (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): AsymAttention(\n",
       "    (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "    (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (ls1): Identity()\n",
       "  (drop_path1): Identity()\n",
       "  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): Mlp(\n",
       "    (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (act): GELU(approximate='none')\n",
       "    (drop1): Dropout(p=0.0, inplace=False)\n",
       "    (norm): Identity()\n",
       "    (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (drop2): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (ls2): Identity()\n",
       "  (drop_path2): Identity()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214cbb88-e13e-4712-89b3-26bacd636a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.conda/envs/memomae/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/sautkin/imagenet1k3?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11.5G/11.5G [01:20<00:00, 154MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/sagemaker-user/.cache/kagglehub/datasets/sautkin/imagenet1k3/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"sautkin/imagenet1k3\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8f21b79-1cf4-43dc-8cd5-2ed2ca3440ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Total images: 326025\n",
      "Train samples: 293423, Val samples: 32602\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"Data/imagenet\")  # change to your dataset root folder\n",
    "train_file = \"train.txt\"\n",
    "val_file = \"val.txt\"\n",
    "val_ratio = 0.1  # 10% validation\n",
    "\n",
    "# Get all class subfolders (sorted alphabetically)\n",
    "class_dirs = sorted([d for d in root.iterdir() if d.is_dir()])\n",
    "\n",
    "samples = []\n",
    "\n",
    "for class_idx, class_dir in enumerate(class_dirs):\n",
    "    for img_path in class_dir.rglob(\"*\"):\n",
    "        if img_path.is_file():\n",
    "\n",
    "            # Include parent folder name in the relative path\n",
    "            # e.g., dataset/00500/img001.jpg\n",
    "            rel_path = img_path.relative_to(root.parent)\n",
    "\n",
    "            # Store (path, numeric_label)\n",
    "            samples.append((str(rel_path), class_idx))\n",
    "\n",
    "# Shuffle\n",
    "random.shuffle(samples)\n",
    "\n",
    "# Split train / val\n",
    "val_size = int(len(samples) * val_ratio)\n",
    "val_samples = samples[:val_size]\n",
    "train_samples = samples[val_size:]\n",
    "\n",
    "# Save train.txt\n",
    "with open(train_file, \"w\") as f:\n",
    "    for path, label in train_samples:\n",
    "        f.write(f\"{path} {label}\\n\")\n",
    "\n",
    "# Save val.txt\n",
    "with open(val_file, \"w\") as f:\n",
    "    for path, label in val_samples:\n",
    "        f.write(f\"{path} {label}\\n\")\n",
    "\n",
    "print(\"Done.\")\n",
    "print(f\"Total images: {len(samples)}\")\n",
    "print(f\"Train samples: {len(train_samples)}, Val samples: {len(val_samples)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f1543-dc8a-4cb1-ade0-766939d5b7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28e100da-9889-4782-9900-7af755f7d673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 60, 192])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(32,1+49*6, 768)\n",
    "x[:, 1:, :][:, ::5, :].shape\n",
    "torch.cat([x[:, :1, :], x[:, 1:, :][:, ::5, :]], dim=1).reshape(32, -1, 4, 768 // 4).permute(0, 2, 1, 3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e307352-92a9-4443-a7ad-ce0ba1480552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ed98fa-a081-43c9-ae7d-e8ae563c4bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6137a577-9197-4dca-b856-02191c18670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = torch.rand(2, 3, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3a70e2b-346c-436a-bfe5-e361eb0a1e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1872, 0.7555, 0.1533, 0.8563, 0.2769],\n",
       "          [0.8628, 0.1221, 0.9076, 0.1022, 0.4352]],\n",
       "\n",
       "         [[0.4514, 0.2249, 0.0550, 0.8790, 0.4933],\n",
       "          [0.5957, 0.7289, 0.7837, 0.4151, 0.9081]],\n",
       "\n",
       "         [[0.7152, 0.0567, 0.0708, 0.7548, 0.4981],\n",
       "          [0.9035, 0.4276, 0.5547, 0.4211, 0.9869]]],\n",
       "\n",
       "\n",
       "        [[[0.3752, 0.8949, 0.3875, 0.9792, 0.0072],\n",
       "          [0.2563, 0.2889, 0.8928, 0.5064, 0.5277]],\n",
       "\n",
       "         [[0.8043, 0.1227, 0.1500, 0.6591, 0.7525],\n",
       "          [0.2815, 0.8226, 0.6769, 0.0271, 0.9653]],\n",
       "\n",
       "         [[0.3970, 0.6205, 0.9092, 0.9966, 0.7645],\n",
       "          [0.6249, 0.0106, 0.9005, 0.1278, 0.9180]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "130cab83-95d6-4fa1-bcd0-efad1d9bd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = x.unsqueeze(1).expand(-1, x.size(1), -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38bbad63-316b-48e4-ab04-bfa9fc88b762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55289617-1593-4f3e-b078-e8993b2269a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5375, 0.7872, 0.8367, 0.5256, 0.1079],\n",
       "         [0.8263, 0.3899, 0.7671, 0.8475, 0.1369],\n",
       "         [0.7796, 0.2523, 0.6017, 0.1883, 0.9118]],\n",
       "\n",
       "        [[0.6196, 0.4296, 0.2355, 0.2263, 0.8392],\n",
       "         [0.8227, 0.4214, 0.5887, 0.5020, 0.8431],\n",
       "         [0.4350, 0.2811, 0.3904, 0.0662, 0.9545]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63be8ab-0801-4259-b6c2-2035bb583123",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sim = torch.cat([out, sim], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63a15503-21c7-4e2e-960e-38639d23e631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5375, 0.7872, 0.8367, 0.5256, 0.1079],\n",
       "          [0.8263, 0.3899, 0.7671, 0.8475, 0.1369],\n",
       "          [0.7796, 0.2523, 0.6017, 0.1883, 0.9118],\n",
       "          [0.1872, 0.7555, 0.1533, 0.8563, 0.2769],\n",
       "          [0.8628, 0.1221, 0.9076, 0.1022, 0.4352]],\n",
       "\n",
       "         [[0.5375, 0.7872, 0.8367, 0.5256, 0.1079],\n",
       "          [0.8263, 0.3899, 0.7671, 0.8475, 0.1369],\n",
       "          [0.7796, 0.2523, 0.6017, 0.1883, 0.9118],\n",
       "          [0.4514, 0.2249, 0.0550, 0.8790, 0.4933],\n",
       "          [0.5957, 0.7289, 0.7837, 0.4151, 0.9081]],\n",
       "\n",
       "         [[0.5375, 0.7872, 0.8367, 0.5256, 0.1079],\n",
       "          [0.8263, 0.3899, 0.7671, 0.8475, 0.1369],\n",
       "          [0.7796, 0.2523, 0.6017, 0.1883, 0.9118],\n",
       "          [0.7152, 0.0567, 0.0708, 0.7548, 0.4981],\n",
       "          [0.9035, 0.4276, 0.5547, 0.4211, 0.9869]]],\n",
       "\n",
       "\n",
       "        [[[0.6196, 0.4296, 0.2355, 0.2263, 0.8392],\n",
       "          [0.8227, 0.4214, 0.5887, 0.5020, 0.8431],\n",
       "          [0.4350, 0.2811, 0.3904, 0.0662, 0.9545],\n",
       "          [0.3752, 0.8949, 0.3875, 0.9792, 0.0072],\n",
       "          [0.2563, 0.2889, 0.8928, 0.5064, 0.5277]],\n",
       "\n",
       "         [[0.6196, 0.4296, 0.2355, 0.2263, 0.8392],\n",
       "          [0.8227, 0.4214, 0.5887, 0.5020, 0.8431],\n",
       "          [0.4350, 0.2811, 0.3904, 0.0662, 0.9545],\n",
       "          [0.8043, 0.1227, 0.1500, 0.6591, 0.7525],\n",
       "          [0.2815, 0.8226, 0.6769, 0.0271, 0.9653]],\n",
       "\n",
       "         [[0.6196, 0.4296, 0.2355, 0.2263, 0.8392],\n",
       "          [0.8227, 0.4214, 0.5887, 0.5020, 0.8431],\n",
       "          [0.4350, 0.2811, 0.3904, 0.0662, 0.9545],\n",
       "          [0.3970, 0.6205, 0.9092, 0.9966, 0.7645],\n",
       "          [0.6249, 0.0106, 0.9005, 0.1278, 0.9180]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb62f55d-760e-4562-a42c-cf5230ac548e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (memomae)",
   "language": "python",
   "name": "memomae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
